Project 1.5
¯¯¯¯¯¯¯¯¯¯¯
Goal: Run some classifiers on the medical dataset.

I first ran the ZeroR classifier on the dataset with a couple unique attributes 
removed (such as Patient ID, etc...).  However, this resulted in a pretty bad error 
rate.  The complete results can be found in the file "ZeroR_initial_result.txt".
The summary of this run was as follows:

=== Summary ===
Correctly Classified Instances       44001               44.0014 %
Incorrectly Classified Instances     55998               55.9986 %
Kappa statistic                          0     
Mean absolute error                      0.0838
Root mean squared error                  0.2046
Relative absolute error                100      %
Root relative squared error            100      %
Total Number of Instances            99999     

I then looked at the visualizer graphs for each attribute and it seemed hat a 
couple were giving the classifier a lot of extra noise.  In order to alleviate 
this, I decided to play around with the data and remove the following fields 
out of curiosity:
Other-Dx-Code-X (with X= 1 -> 14)
Other-PR-Code-X (with X= 1 -> 14)
CMCX (with X= 2 -> 10)
PLX (with X= 2 -> 5)

However, the classifier still had the same results:

=== Summary ===

Correctly Classified Instances       44001               44.0014 %
Incorrectly Classified Instances     55998               55.9986 %
Kappa statistic                          0     
Mean absolute error                      0.0838
Root mean squared error                  0.2046
Relative absolute error                100      %
Root relative squared error            100      %
Total Number of Instances            99999 

I then looked at the confusion matrix, and found that it was classifying everything 
as disposition code 1.  This was the most common disposition code, however obviously 
the classifier wasn't finding a very good way to classify patients.

For the purposes of our experiment, it wasn't very useful to have a classifier that 
could classify EVERY disposition code available.  Our project questions were mainly 
concerned with finding a way to predict the disposition codes of 
20 (Hospice - Medical Facility) and 2 (Other Acute Care Hospital).  In order to 
reduce the amount of noise in the dataset, perhaps the classifications could be 
reduced to simply: 20, 2, or anything else.

To fix the problem, I converted the train-a.arff file into a csv file, after which 
I wrote a quick perl script to do the find/replacement of any codes that were not 
2 or 20 with a 0 (octal \060 in the regex), and to leave all other fields and 
lines alone.  I then ran the perl script on the csv file to create another 
reduced csv file.  Weka was able to read the new file and convert it to arff format 
fine.  The perl script is called "reduce_disp_codes.pl".
The next time I ran the ZeroR classifier again, it gave me this:

=== Classifier model (full training set) ===

ZeroR predicts class value: 0.5835558355583556

Time taken to build model: 0.03 seconds

=== Cross-validation ===
=== Summary ===

Correlation coefficient                 -0.0055
Mean absolute error                      1.0829
Root mean squared error                  3.3144
Relative absolute error                100      %
Root relative squared error            100      %
Total Number of Instances            99999  

WEKA was predicting an invalid floating point value of 0.583 for the disposition 
code! Because I had converted to a CSV file, the previous NumericToNominal 
processing that I had done in project 1.5 was lost!  I simply re-ran my filter 
commands to re-nominalize the nominal fields:

weka.filters.unsupervised.attribute.NumericToNominal -R 7-9,11-13,15-16,31-45,48-49,53,56-71,77-78

After this, the ZeroR classifier's accuracy improved amazingly!
The full run buffer was saved to "ZeroR_reduced_result.txt".  

=== Classifier model (full training set) ===

ZeroR predicts class value: 0

Time taken to build model: 0.09 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances       93593               93.5939 %
Incorrectly Classified Instances      6406                6.4061 %
Kappa statistic                          0     
Mean absolute error                      0.0812
Root mean squared error                  0.2015
Relative absolute error                100      %
Root relative squared error            100      %
Total Number of Instances            99999  

The classifier was able to correctly classify the opposite of our disposition codes 
of interest.  That is to say: any code EXCEPT 2 OR 20 can be classified 93.59% of 
the time.

Next, I decided to do a quick experiment comparing some other classifiers to the 
ZeroR one.  I opened the WEKA experimenter and tried to compare the ZeroR, J48 and 
BFTree classifiers, however I kept getting the error: "Class attribute is not nominal!".
From this link, I found that in the experimenter, the class attribute is by default 
assumed to be the last one: 
https://list.scms.waikato.ac.nz/pipermail/wekalist/2002-April/000222.html

I opened the csv file in excel and copied/pasted the "I" column which cooresponded 
to the Patient-Disposition to the very end.  After waiting for a bit, I saved as a 
new CSV file.  Again, I ran the NumericToNominal command in WEKA's explorer, and 
tried my experiment a second time.

Again, I got the same output:

01:54:44: Started
01:54:48: Class attribute is not nominal!
01:54:48: Interrupted
01:54:48: There was 1 error

After looking at the data again, I found that since the order of attributes changed, 
the Patient-Disposition field's index had changed (along with any attribute with 
index >8).  The new NumericToNominal line I needed to run was:

weka.filters.unsupervised.attribute.NumericToNominal -R 7-8,10-12,14-29,30-44,47-48,52,55-70,76-77,79

The experiment started running.... but yet again WEKA crashed eventually with 
the ever-present out of memory error :(