Project 1.6
¯¯¯¯¯¯¯¯¯¯¯
Goal: Apply what we've learned to date to the weather dataset (exploration, summary stats, clustering, classification tree).
      Experiment using Weka's rule-based and nearest-neighbor classifiers.

First I opened the original weather dataset and found the following summary stats:

Outlook:
<label>		<frequency>
sunny		5
overcast	4
rainy		5

Humidity:
Minimum:	65
Maximum:	96
Mean:		81.643
StdDev:		10.285

Temperature:
Minimum:	64
Maximum:	85
Mean:		73.571
StdDev:		6.572

Windy:
<label>		<frequency>
TRUE		6
FALSE		8

Play:
<label>		<frequency>
yes			9
no			5

Afterwards, I ran the simple K-means clusterer with N=2 on the set, and saw 
that the initial clusters it found didn't make all that much sense.  Graphing the 
cluster # vs various attributes, I saw that the clusterer mainly seemed to group 
records by whether or not it was windy and temperature.  There were 5 records that 
were grouped together all with windy=true and temperatures rather close to the 
centroid of 69.4.  The rest of the records all had windy=false and temperatures 
that seemed to range widely.  However, they also seemed to be grouped by humidity 
as well.  Nothing really useful came out of these clusterings.

=== Run information ===

Scheme:       weka.clusterers.SimpleKMeans -N 2 -A "weka.core.EuclideanDistance -R first-last" -I 500 -S 10
Relation:     weather
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    evaluate on training data



=== Model and evaluation on training set ===


kMeans
======

Number of iterations: 3
Within cluster sum of squared errors: 16.237456311387238
Missing values globally replaced with mean/mode

Cluster centroids:
                           Cluster#
Attribute      Full Data          0          1
                    (14)        (9)        (5)
==============================================
outlook            sunny      sunny   overcast
temperature      73.5714    75.8889       69.4
humidity         81.6429    84.1111       77.2
windy              FALSE      FALSE       TRUE
play                 yes        yes        yes


Clustered Instances

0       9 ( 64%)
1       5 ( 36%)

After looking at the cluster analysis quickly, I moved on to do an experiment with 
3 rule based classifiers.  I created & ran the experiment found in file 
"Rule_Classifier_experiment.exp" with WEKA's Experimenter GUI.  After the classifiers 
ran, I looked at the results to see which rule based classifier of the 3 was best.

Classifiers ran:
(1) rules.NNge '-G 5 -I 5' 4084742275553788972
(2) rules.Ridor '-F 3 -S 1 -N 2.0' -7261533075088314436
(3) rules.JRip '-F 3 -N 2.0 -O 2 -S 1' -6589312996832147161

From the results found in "Rule_classifier_percent_correct_results.txt", it seems 
that out of the 3 chosen classifiers, WEKA considered them statistically to be 
equal.  At the bottom of the results table, it shows each set run was not 
significantly better or worse than the others in predicting the class variable "play" 
as evidenced by the "(0/1/0)" indicator at the bottom for each.  The interesting 
thing is that the best percentage correct was won by the JRip classifier, 
which simply had a single, yet funny rule.  However, as all 3 classifiers were in 
the same ball park, it seems that they all were equally bad at generating great 
rules to tell whether or not to play golf.  I believe that with a larger dataset, 
perhaps these algorithms would fare better.

Dataset                   (1) rules.NN | (2) rules (3) rules
------------------------------------------------------------
weather                  (100)   66.00 |   55.50     67.50  
------------------------------------------------------------
                               (v/ /*) |   (0/1/0)   (0/1/0)

Explanations of classifiers (per WEKA's documentation):
rules.NNge - Nearest -neighbor method of generating rules 
			 using nonnested generalized exemplars. 
			 (which are hyperrectangles that can be viewed as if-then rules)
rules.Ridor - An implementation of a RIpple-DOwn Rule learner.
rules.JRip - This class implements a propositional rule learner, 
			 Repeated Incremental Pruning to Produce Error 
			 Reduction (RIPPER), which was proposed by William W.

The detailed results of the NNge classifier were interesting because they displayed 
the rules as if then statements (which WEKA seems to call "exemplars").  There 
were 5 rules generated by this classifier.

-------------------------------------------------------------------------------
=== Run information ===

Scheme:       weka.classifiers.rules.NNge -G 5 -I 5
Relation:     weather
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    10-fold cross-validation

=== Classifier model (full training set) ===


NNGE classifier

Rules generated :
	class no IF : outlook in {rainy} ^ 65.0<=temperature<=71.0 ^ 70.0<=humidity<=91.0 ^ windy in {TRUE}  (2)
	class yes IF : outlook in {overcast} ^ temperature=72.0 ^ humidity=90.0 ^ windy in {TRUE}  (1)
	class yes IF : outlook in {overcast,rainy} ^ 68.0<=temperature<=83.0 ^ 75.0<=humidity<=96.0 ^ windy in {FALSE}  (5)
	class yes IF : outlook in {sunny,overcast} ^ 64.0<=temperature<=75.0 ^ 65.0<=humidity<=70.0 ^ windy in {TRUE,FALSE}  (3)
	class no IF : outlook in {sunny} ^ 72.0<=temperature<=85.0 ^ 85.0<=humidity<=95.0 ^ windy in {TRUE,FALSE}  (3)

Stat :
	class yes : 3 exemplar(s) including 2 Hyperrectangle(s) and 1 Single(s).
	class no : 2 exemplar(s) including 2 Hyperrectangle(s) and 0 Single(s).

	Total : 5 exemplars(s) including 4 Hyperrectangle(s) and 1 Single(s).

	Feature weights : [0.24674981977443894 0.19996253177061085 0.21886699651992553 0.04812703040826924]



Time taken to build model: 0 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances           8               57.1429 %
Incorrectly Classified Instances         6               42.8571 %
Kappa statistic                         -0.0244
K&B Relative Info Score                107.2116 %
K&B Information Score                    1.041  bits      0.0744 bits/instance
Class complexity | order 0              13.7612 bits      0.9829 bits/instance
Class complexity | scheme             6444      bits    460.2857 bits/instance
Complexity improvement     (Sf)      -6430.2388 bits   -459.3028 bits/instance
Mean absolute error                      0.4286
Root mean squared error                  0.6547
Relative absolute error                 90      %
Root relative squared error            132.6919 %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.778     0.8        0.636     0.778     0.7        0.489    yes
                 0.2       0.222      0.333     0.2       0.25       0.489    no
Weighted Avg.    0.571     0.594      0.528     0.571     0.539      0.489

=== Confusion Matrix ===

 a b   <-- classified as
 7 2 | a = yes
 4 1 | b = no
-------------------------------------------------------------------------------


The Ridor rule learner surprisingly shows it's output simply as 2 simple rules.
It seems to start with a default rule with 1 exception as follows:
play = no  (14.0/9.0)
           Except (humidity <= 82.5) => play = yes  (5.0/1.0) [2.0/0.0]

-------------------------------------------------------------------------------
=== Run information ===

Scheme:       weka.classifiers.rules.Ridor -F 3 -S 1 -N 2.0
Relation:     weather
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    10-fold cross-validation

=== Classifier model (full training set) ===

RIpple DOwn Rule Learner(Ridor) rules
--------------------------------------

play = no  (14.0/9.0)
           Except (humidity <= 82.5) => play = yes  (5.0/1.0) [2.0/0.0]

Total number of rules (incl. the default rule): 2

Time taken to build model: 0 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances           5               35.7143 %
Incorrectly Classified Instances         9               64.2857 %
Kappa statistic                         -0.4651
K&B Relative Info Score               -542.1116 %
K&B Information Score                   -5.2636 bits     -0.376  bits/instance
Class complexity | order 0              13.7612 bits      0.9829 bits/instance
Class complexity | scheme             9666      bits    690.4286 bits/instance
Complexity improvement     (Sf)      -9652.2388 bits   -689.4456 bits/instance
Mean absolute error                      0.6429
Root mean squared error                  0.8018
Relative absolute error                135      %
Root relative squared error            162.5137 %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.556     1          0.5       0.556     0.526      0.278    yes
                 0         0.444      0         0         0          0.278    no
Weighted Avg.    0.357     0.802      0.321     0.357     0.338      0.278

=== Confusion Matrix ===

 a b   <-- classified as
 5 4 | a = yes
 5 0 | b = no
 -------------------------------------------------------------------------------
 
 
 Finally, the JRip classifier seemed to have the simplest ruleset, with only 1 rule:
 
 => play=yes (14.0/5.0)
 
 Obviously this probably is not a favorable way to decide whether or not to play 
 golf, however, it's the most accurate when considering my personal wish to be able 
 to play golf all the time ^_^
 
 This most likely resulted due to the small number of data points in this set, as 
 well as the rule classifier's attempt at pruning for error reduction.  I'm sure 
 that if the dataset were larger, pruning would be more necessary, however with this 
 small set, it simply results in a comical rule.  The other funny thing is that 
 out of the 3 classifiers I chose, using a T-test with a confidence interval of 95%
 (significance in WEKA was set to: 0.05), JRip had the best average error rate of
 67.50 percent correct.
 
 What I learned:  If there's ever a need for a statistical proof that you should 
 play golf all the time, then this is it!
 
 -------------------------------------------------------------------------------
 === Run information ===

Scheme:       weka.classifiers.rules.JRip -F 3 -N 2.0 -O 2 -S 1
Relation:     weather
Instances:    14
Attributes:   5
              outlook
              temperature
              humidity
              windy
              play
Test mode:    10-fold cross-validation

=== Classifier model (full training set) ===

JRIP rules:
===========

 => play=yes (14.0/5.0)

Number of Rules : 1


Time taken to build model: 0 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances           8               57.1429 %
Incorrectly Classified Instances         6               42.8571 %
Kappa statistic                         -0.1351
K&B Relative Info Score               -226.918  %
K&B Information Score                   -2.2033 bits     -0.1574 bits/instance
Class complexity | order 0              13.7612 bits      0.9829 bits/instance
Class complexity | scheme             2161.5038 bits    154.3931 bits/instance
Complexity improvement     (Sf)      -2147.7426 bits   -153.4102 bits/instance
Mean absolute error                      0.5516
Root mean squared error                  0.6149
Relative absolute error                115.8462 %
Root relative squared error            124.6318 %
Total Number of Instances               14     

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.889     1          0.615     0.889     0.727      0.122    yes
                 0         0.111      0         0         0          0.122    no
Weighted Avg.    0.571     0.683      0.396     0.571     0.468      0.122

=== Confusion Matrix ===

 a b   <-- classified as
 8 1 | a = yes
 5 0 | b = no